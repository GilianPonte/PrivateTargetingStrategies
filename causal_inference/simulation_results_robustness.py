# -*- coding: utf-8 -*-
"""simulation_results_robustness.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TL-wGCCXKNrKfeSiQRhMSpWssWtXkffn
"""

!pip uninstall tensorflow -y
!pip install tensorflow==2.15
!pip install tensorflow-privacy
!pip install keras_tuner

!pip install git+https://github.com/GilianPonte/PrivateTargetingStrategies.git -q

import tensorflow_privacy
from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer
import keras_tuner

def pcnn(X, Y, T, scaling=True, batch_size=100, epochs=100, max_epochs=1, directory="tuner", fixed_model = False, noise_multiplier= None, l2_norm_clip =4, eta = 0.001, seed = None):
    """
    Private Causal Neural Network (PCNN) algorithm for estimating average treatment effects.

    Args:
    X (numpy.ndarray): Features matrix.
    Y (numpy.ndarray): Outcome vector.
    T (numpy.ndarray): Treatment vector.
    scaling (bool, optional): Whether to scale the features matrix. Default is True.
    simulations (int, optional): Number of simulations. Default is 1.
    batch_size (int, optional): Batch size for training. Default is 100.
    epochs (int, optional): Number of epochs for training. Default is 100.
    max_epochs (int, optional): Maximum number of epochs for hyperparameter optimization. Default is 10.
    directory (str, optional): Directory for saving hyperparameter optimization results. Default is "tuner".
    noise_multiplier (float, optional): Noise multiplier for differential privacy. Default is 1.

    Returns:
    tuple: Tuple containing average treatment effect, CATE estimates, trained tau_hat model, and privacy risk .
    """

    import random
    import re
    import numpy as np
    import tensorflow as tf
    import tensorflow_privacy
    from tensorflow import keras
    from keras.layers import Activation, LeakyReLU
    from keras import backend as K
    from keras import layers
    from keras.utils import get_custom_objects
    from sklearn.model_selection import KFold
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.linear_model import LogisticRegression
    import pandas as pd
    import math
    import keras_tuner

    # set random seeds
    np.random.seed(seed)
    tf.random.set_seed(seed)
    random.seed(seed)
    tf.keras.utils.set_random_seed(seed)

    # Check if batch size divides the data evenly
    if (len(X)/2) % batch_size != 0:
        divisors = [i for i in range(1, int(math.sqrt((len(X)/2))) + 1) if (len(X)/2) % i == 0]
        divisors += [(len(X)/2) // i for i in divisors if (len(X)/2) // i != i]
        divisors.sort()
        raise ValueError("The batch size you have specified does not divide the data into a whole number.\nPlease select one of the following possible batch sizes: {}".format(np.round(divisors)))

    # Calculate epsilon
    statement = tensorflow_privacy.compute_dp_sgd_privacy_statement(
        number_of_examples=len(X),
        batch_size=batch_size,
        num_epochs=epochs,
        noise_multiplier=noise_multiplier,
        delta=1/len(X),
        used_microbatching=False,
        max_examples_per_user=1
    )
    print(statement)

    # Extract epsilon and noise_multiplier from the statement
    numbers = [float(num) if '.' in num else int(num) for num in re.findall(r'\d+\.\d+|\d+', statement)]
    n, epsilon, noise_multiplier, epsilon_conservative = numbers[0], numbers[8], numbers[2], numbers[7]


    # callback settings for early stopping and saving
    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, mode="min")  # early stopping just like in rboost

    # define ate loss is equal to mean squared error between pseudo outcome and prediction of net.
    def ATE(y_true, y_pred):
        return tf.reduce_mean(y_pred, axis=-1)  # Note the `axis=-1`

    def generate_fixed_architecture(X):
      model = keras.Sequential()
      model.add(keras.Input(shape=(X.shape[1],)))

      # Define the architecture with 4 layers
      num_layers = 4
      units = 64

      for _ in range(num_layers):
          model.add(layers.Dense(units, activation='tanh')) # https://arxiv.org/pdf/2007.14191.pdf
          units = max(units // 2, 1)  # Reduce the number of units by half for each subsequent layer

      # Add output layer
      model.add(layers.Dense(1, activation='linear'))

      model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss="mean_squared_error",
        metrics=["MSE"],
      )
      return model

    average_treatment_effect = []  # storage of ate estimates
    all_CATE_estimates = []  # Store CATE estimates for each simulation

    ## scale the data for well-behaved gradients
    if scaling == True:
        scaler0 = MinMaxScaler(feature_range=(-1, 1))
        scaler0 = scaler0.fit(X)
        X = scaler0.transform(X)
        X = pd.DataFrame(X)

    ## Add leaky-relu so we can use it as a string
    get_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=0.2))})

    def build_model(hp):
        model = keras.Sequential()
        model.add(keras.Input(shape=(X.shape[1],)))
        # Tune the number of layers.
        for i in range(hp.Int("num_layers", 1, 4)):
            model.add(
                layers.Dense(
                    # Tune number of units separately.
                    units=hp.Choice(f"units_{i}", [8, 16, 32, 64, 256, 512]),
                    activation=hp.Choice("activation", ["leaky-relu", "relu"]),
                )
            )
        model.add(layers.Dense(1, activation="linear"))

        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss="mean_squared_error",
            metrics=["MSE"],
        )
        return model

    # for epsilon calculation. Shuffeling serves as a Poisson sampling substitute (Ponomareva et al. 2023)
    idx = np.random.permutation(pd.DataFrame(X).index)
    X = np.array(pd.DataFrame(X).reindex(idx))
    Y = np.array(pd.DataFrame(Y).reindex(idx))
    T = np.array(pd.DataFrame(T).reindex(idx))

    # save models
    checkpoint_filepath_mx = f"{directory}/_{epsilon}_m_x.hdf5"
    checkpoint_filepath_taux = f"{directory}/_{epsilon}_tau_x.hdf5"

    mx_callbacks = [callback,
      tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath_mx, save_weights_only=False, monitor='val_loss', mode='min', save_freq="epoch", save_best_only=True,)]
    tau_hat_callbacks = [callback,
      tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath_taux, save_weights_only=False, monitor='val_loss', mode='min', save_freq="epoch", save_best_only=True,)]
    y_tilde_hat = []  # collect all the \tilde{Y}
    T_tilde_hat = []  # collect all the \tilde{T}
    m_x_hat = []  # collect all m_x_hat for print
    e_x_hat = []  # collect all e_x_hat for print

    print("hyperparameter optimization for yhat")
    tuner = keras_tuner.Hyperband(hypermodel=build_model, objective="val_loss", max_epochs=max_epochs, overwrite=True, directory=directory, project_name="yhat",seed=seed,) # random search is at least as slow..

    tuner.search(X, Y, epochs=epochs, validation_split=0.25, verbose=0, callbacks=[mx_callbacks])
    # Get the optimal hyperparameters
    best_hps = tuner.get_best_hyperparameters()[0]
    print("the optimal architecture is: " + str(best_hps.values))

    cv = KFold(n_splits=2, shuffle=False)  # K-fold validation shuffle is off to prevent additional noise?

    for fold, (train_idx, test_idx) in enumerate(cv.split(X)):
      # set random seeds
      np.random.seed(seed)
      tf.random.set_seed(seed)
      random.seed(seed)
      tf.keras.utils.set_random_seed(seed)

      # training model for m(x)
      model_m_x = tuner.hypermodel.build(best_hps)
      model_m_x.fit(X[train_idx],
                    Y[train_idx],
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_data=(X[test_idx], Y[test_idx]),
                    callbacks=mx_callbacks,
                    verbose=0)
      model_m_x = tuner.hypermodel.build(best_hps)
      model_m_x.build(input_shape=(None, X.shape[1]))
      model_m_x.load_weights(checkpoint_filepath_mx)
      m_x = model_m_x.predict(x=X[test_idx], verbose=0).reshape(len(Y[test_idx]))  # obtain \hat{m}(x) from test set

      # obtain \tilde{Y} = Y_{i} - \hat{m}(x)
      truth = Y[test_idx].T.reshape(len(Y[test_idx]))
      y_tilde = truth - m_x
      y_tilde_hat = np.concatenate((y_tilde_hat, y_tilde))  # cbind in r
      m_x_hat = np.concatenate((m_x_hat, m_x))  # cbind in r

      # fit \hat{e}(x)
      clf = LogisticRegression(verbose=0).fit(X[train_idx], np.array(T[train_idx]).reshape(len(T[train_idx])))
      e_x = clf.predict_proba(X[test_idx])  # obtain \hat{e}(x)
      print(f"Fold {fold}: mean(m_x) = {np.round(np.mean(m_x), 2):.2f}, sd(m_x) = {np.round(np.std(m_x), 3):.3f} and mean(e_x) = {np.round(np.mean(e_x[:, 1]), 2):.2f}, sd(e_x) = {np.round(np.std(e_x[:, 1]), 3):.3f}")

      # obtain \tilde{T} = T_{i} - \hat{e}(x)
      truth = T[test_idx].T.reshape(len(T[test_idx]))
      T_tilde = truth - e_x[:, 1]
      T_tilde_hat = np.concatenate((T_tilde_hat, T_tilde))
      e_x_hat = np.concatenate((e_x_hat, e_x[:, 1]))

    # storage
    CATE_estimates = []

    # pseudo_outcome and weights
    pseudo_outcome = (y_tilde_hat / T_tilde_hat)  # pseudo_outcome = \tilde{Y} / \tilde{T}
    w_weights = np.square(T_tilde_hat)  # \tilde{T}**2

    cv = KFold(n_splits=2, shuffle=False)

    print("training for tau hat")
    for fold, (train_idx, test_idx) in enumerate(cv.split(X)):
      # set random seeds
      np.random.seed(seed)
      tf.random.set_seed(seed)
      random.seed(seed)
      tf.keras.utils.set_random_seed(seed)

      tau_hat = generate_fixed_architecture(X) # an alternative is to fix the values of hyperparameters to some reasonable defaults and forgo hyperparameter tuning altogether (Ponomareva et al. 2023)
      if fixed_model == False:
        tau_hat = tuner.hypermodel.build(best_hps)
      tau_hat.compile(optimizer=tensorflow_privacy.DPKerasAdamOptimizer(l2_norm_clip=l2_norm_clip, noise_multiplier=noise_multiplier, num_microbatches=batch_size, learning_rate=eta),
                      loss=tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.NONE), metrics=[ATE]) # the microbatches are equal to the batch size. No microbatching applied.
      history_tau = tau_hat.fit(
        X[train_idx],
        pseudo_outcome[train_idx],
        sample_weight=w_weights[train_idx],
        epochs=epochs,
        batch_size=batch_size,
        callbacks=tau_hat_callbacks,
        validation_data=(X[test_idx], pseudo_outcome[test_idx]),
        verbose=0)
      if fixed_model == False:
        tau_hat = tuner.hypermodel.build(best_hps)
        tau_hat.build(input_shape=(None, X.shape[1]))
      tau_hat.load_weights(checkpoint_filepath_taux)
      CATE = tau_hat.predict(x=X[test_idx], verbose=0).reshape(len(X[test_idx]))
      print(f"Fold {fold}: mean(tau_hat) = {np.round(np.mean(CATE), 2):.2f}, sd(tau_hat) = {np.round(np.std(CATE), 3):.3f}")

      CATE_estimates = np.concatenate((CATE_estimates, CATE))  # store CATE's
    average_treatment_effect = np.mean(CATE_estimates)
    X = X[np.argsort(idx)]
    Y = Y[np.argsort(idx)]
    T = T[np.argsort(idx)]
    CATE_estimates = CATE_estimates[np.argsort(idx)]
    print(f"ATE = {average_treatment_effect}")
    return average_treatment_effect, CATE_estimates, tau_hat, n, epsilon, noise_multiplier, epsilon_conservative

import random
import numpy as np
def generate_and_write_seeds(file_path, num_seeds, seed=None):
    # If a seed is provided, use it for reproducibility
    if seed is not None:
        random.seed(seed)

    # Generate random seeds deterministically
    seeds = [random.getrandbits(32) for _ in range(num_seeds)]

    # Write seeds to a text file
    with open(file_path, "w") as file:
        for seed in seeds:
            file.write(str(seed) + "\n")

    print("Seeds have been written to", file_path)

def read_file(file_path):
    with open(file_path, "r") as file:
        return [int(seed.strip()) for seed in file.readlines()]

# Set the initial seed for reproducibility
initial_seed = 422312

# Generate and write seeds for seeds_data.txt
generate_and_write_seeds("seeds_data.txt", 100, seed=initial_seed)
generate_and_write_seeds("seeds_training.txt", 700, seed=initial_seed)

# install PCNNs and simulation setup
#!pip install git+https://github.com/GilianPonte/PrivateTargetingStrategies.git -q

import os
import pandas as pd
import numpy as np
import random
import tensorflow
import causal_inference
from causal_inference import simulation_data
from causal_inference import strategy1
import time

try:
    seeds_data = np.genfromtxt('/content/seeds_data.txt', delimiter=',', dtype = np.int64)
except IOError:
    print("Error: File not found or could not be read.")
try:
    seeds_training = np.genfromtxt('/content/seeds_training.txt', delimiter=',', dtype = np.int64)
except IOError:
    print("Error: File not found or could not be read.")

# set time
start_time = time.time()
tensorflow.config.experimental.enable_op_determinism()
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

# Read seeds_data and seeds_training from file
seeds_data = read_file("seeds_data.txt")
seeds_training = read_file("seeds_training.txt")

# simulation parameters
iterations = 1
results_list = []
l2normclips = [2, 4, 8, 12, 20] # Initialize lists to store results for each noise multiplier 0.48033, 0.7174, 1.5088, 3.5953, 6.5978, 52.174, 204,

for i in range(iterations):
  print("Iteration: {}".format(i, i))
  random.seed(seeds_data[i])
  tensorflow.random.set_seed(seeds_data[i])
  np.random.seed(seeds_data[i])
  tensorflow.keras.utils.set_random_seed(seeds_data[i])

  # read data
  data = simulation_data.data_simulation(10_000)

  # Separate the columns into different DataFrames
  x = data[['covariate_1', 'covariate_2', 'covariate_3', 'covariate_4', 'covariate_5', 'covariate_6']]
  w, m, tau, mu1, mu0, y = data[['w']], data[['m']], data[['tau']], data[['mu1']], data[['mu0']], data[['y']]

  # Loop through each noise multiplier value
  for noise_index, normclip in enumerate(l2normclips):
    print(noise_multiplier)

    for a in range(10):
      combined_number = (noise_index * 100) + a
      print("Combined number: {}".format(combined_number, combined_number))
      random.seed(seeds_training[combined_number])
      tensorflow.random.set_seed(seeds_training[combined_number])
      np.random.seed(seeds_training[combined_number])
      tensorflow.keras.utils.set_random_seed(seeds_training[combined_number])

      # Define the directory based on the noise multiplier
      directory = f"tuner_{noise_multiplier}_iteration_{i}_algo_run_{a}_1"
      os.makedirs(directory, exist_ok=True)  # Create the directory if it doesn't exist
      average_treatment_effect, CATE_estimates, tau_hat, n, epsilon, noise_multiplier, epsilon_conservative = pcnn(
            X=x,
            Y=y,
            T=w,
            scaling=True,
            batch_size=100,
            epochs=100,
            max_epochs=1,
            fixed_model = True,
            directory=directory,  # Use the directory variable here
            noise_multiplier= 3.5953,
            l2_norm_clip = normclip,
            eta = 0.001,
            seed = seeds_training[combined_number]
      )
      # Append the results to the list
      results_list.append({
          'Noise Multiplier': 3.5953,
          'Average Treatment Effect': average_treatment_effect,
          'CATE Estimates': CATE_estimates,
          'Epsilon': epsilon,
          'epsilon conservative': epsilon_conservative,
          'true ate': data[['tau']].mean(),
          'true CATE': data[['tau']],
          'sample size': n,
          'data set' : i,
          'iteration' : a,
          'covariates' : x,
          'l2norm' : normclip,

          })

      np.save(f"results_list_{normclip}", results_list)

      # Print or use the DataFrames as needed
      print(results_list)
      end_time = time.time()
      execution_time = end_time - start_time
      print("Execution time one sim: {:.2f} seconds".format(execution_time))

import numpy as np
import pandas as pd
import glob

# List of file paths to process
file_paths = [
    "results_list_2.npy",
    "results_list_4.npy",
    "results_list_8.npy",
    "results_list_12.npy",
    "results_list_20.npy",
]

# Initialize an empty list to store all expanded data
all_expanded_data = []

# Loop through each file and process it
for file_path in file_paths:
    try:
        # Load the .npy file
        data = np.load(file_path, allow_pickle=True)

        # Expand the data
        for entry in data:
            l2_norm = entry["l2norm"]
            iteration = entry["iteration"]
            noise_multiplier = entry["Noise Multiplier"]
            true_cate_df = entry["true CATE"]  # DataFrame with 10,000 rows
            cate_estimates = entry["CATE Estimates"]  # NumPy array with same length

            # Ensure alignment between True CATE and CATE Estimates
            for true_cate_val, cate_estimate_val in zip(true_cate_df.values.flatten(), cate_estimates):
                all_expanded_data.append({
                    "File": file_path.split("/")[-1],  # Store filename for reference
                    "L2 Norm": l2_norm,
                    "Iteration": iteration,
                    "True CATE": true_cate_val,
                    "CATE Estimate": cate_estimate_val,
                    "Noise Multiplier": noise_multiplier,
                })
    except Exception as e:
        print(f"Error processing file {file_path}: {e}")

# Convert the collected data into a DataFrame
df_all_expanded = pd.DataFrame(all_expanded_data)

from sklearn.metrics import mean_squared_error
import scipy.stats as stats

# Group by L2 Norm and compute MSE between True CATE and CATE Estimate
def mse_with_ci(group, confidence=0.95):
    errors = (group["True CATE"] - group["CATE Estimate"]) ** 2
    mse = errors.mean()
    se = errors.std(ddof=1) / np.sqrt(len(errors))  # Standard error
    z = stats.norm.ppf(1 - (1 - confidence) / 2)  # Z-score for CI
    ci_lower, ci_upper = mse - z * se, mse + z * se
    return pd.Series({"MSE": mse, "SE": se, "CI Lower": ci_lower, "CI Upper": ci_upper})

results = df_all_expanded.groupby(["L2 Norm"]).apply(mse_with_ci).reset_index()

results

"""# learning rate"""

# install PCNNs and simulation setup
#!pip install git+https://github.com/GilianPonte/PrivateTargetingStrategies.git -q

import os
import pandas as pd
import numpy as np
import random
import tensorflow
import causal_inference
from causal_inference import simulation_data
from causal_inference import strategy1
import time

try:
    seeds_data = np.genfromtxt('/content/seeds_data.txt', delimiter=',', dtype = np.int64)
except IOError:
    print("Error: File not found or could not be read.")
try:
    seeds_training = np.genfromtxt('/content/seeds_training.txt', delimiter=',', dtype = np.int64)
except IOError:
    print("Error: File not found or could not be read.")

# set time
start_time = time.time()
tensorflow.config.experimental.enable_op_determinism()
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

# Read seeds_data and seeds_training from file
seeds_data = read_file("seeds_data.txt")
seeds_training = read_file("seeds_training.txt")

# simulation parameters
iterations = 1
results_list = []
learning_rates = [0.1, 1e2, 1e-3, 1e4, 1e-5] # Initialize lists to store results for each noise multiplier 0.48033, 0.7174, 1.5088, 3.5953, 6.5978, 52.174, 204,

for i in range(iterations):
  print("Iteration: {}".format(i, i))
  random.seed(seeds_data[i])
  tensorflow.random.set_seed(seeds_data[i])
  np.random.seed(seeds_data[i])
  tensorflow.keras.utils.set_random_seed(seeds_data[i])

  # read data
  data = simulation_data.data_simulation(10_000)

  # Separate the columns into different DataFrames
  x = data[['covariate_1', 'covariate_2', 'covariate_3', 'covariate_4', 'covariate_5', 'covariate_6']]
  w, m, tau, mu1, mu0, y = data[['w']], data[['m']], data[['tau']], data[['mu1']], data[['mu0']], data[['y']]

  # Loop through each noise multiplier value
  for noise_index, learning_rate in enumerate(learning_rates):
    print(learning_rate)

    for a in range(10):
      combined_number = (noise_index * 100) + a
      print("Combined number: {}".format(combined_number, combined_number))
      random.seed(seeds_training[combined_number])
      tensorflow.random.set_seed(seeds_training[combined_number])
      np.random.seed(seeds_training[combined_number])
      tensorflow.keras.utils.set_random_seed(seeds_training[combined_number])

      # Define the directory based on the noise multiplier
      directory = f"tuner_{noise_multiplier}_iteration_{i}_algo_run_{a}_1"
      os.makedirs(directory, exist_ok=True)  # Create the directory if it doesn't exist
      average_treatment_effect, CATE_estimates, tau_hat, n, epsilon, noise_multiplier, epsilon_conservative = pcnn(
            X=x,
            Y=y,
            T=w,
            scaling=True,
            batch_size=100,
            epochs=100,
            max_epochs=1,
            fixed_model = True,
            directory=directory,  # Use the directory variable here
            noise_multiplier= 3.5953,
            l2_norm_clip = 4,
            eta = learning_rate,
            seed = seeds_training[combined_number]
      )
      # Append the results to the list
      results_list.append({
          'Noise Multiplier': 3.5953,
          'Average Treatment Effect': average_treatment_effect,
          'CATE Estimates': CATE_estimates,
          'Epsilon': epsilon,
          'epsilon conservative': epsilon_conservative,
          'true ate': data[['tau']].mean(),
          'true CATE': data[['tau']],
          'sample size': n,
          'data set' : i,
          'iteration' : a,
          'covariates' : x,
          'l2norm' : 4,
          'learning_rate' : learning_rate,
          })

      np.save(f"results_list_{learning_rate}", results_list)

      # Print or use the DataFrames as needed
      print(results_list)
      end_time = time.time()
      execution_time = end_time - start_time
      print("Execution time one sim: {:.2f} seconds".format(execution_time))

import numpy as np
import pandas as pd
import glob

# List of file paths to process
file_paths = [
    "results_list_0.001.npy",
    "results_list_0.01.npy",
    "results_list_0.1.npy",
    "results_list_0.0001.npy",
    "results_list_1e-05.npy",
    "results_list_1e-06.npy",
]

# Initialize an empty list to store all expanded data
all_expanded_data = []

# Loop through each file and process it
for file_path in file_paths:
    try:
        # Load the .npy file
        data = np.load(file_path, allow_pickle=True)

        # Expand the data
        for entry in data:
            learning_rate = entry["learning_rate"]
            iteration = entry["iteration"]
            noise_multiplier = entry["Noise Multiplier"]
            true_cate_df = entry["true CATE"]  # DataFrame with 10,000 rows
            cate_estimates = entry["CATE Estimates"]  # NumPy array with same length

            # Ensure alignment between True CATE and CATE Estimates
            for true_cate_val, cate_estimate_val in zip(true_cate_df.values.flatten(), cate_estimates):
                all_expanded_data.append({
                    "File": file_path.split("/")[-1],  # Store filename for reference
                    "learning_rate": learning_rate,
                    "Iteration": iteration,
                    "True CATE": true_cate_val,
                    "CATE Estimate": cate_estimate_val,
                    "Noise Multiplier": noise_multiplier,
                })
    except Exception as e:
        print(f"Error processing file {file_path}: {e}")

# Convert the collected data into a DataFrame
df_all_expanded = pd.DataFrame(all_expanded_data)

from sklearn.metrics import mean_squared_error
import scipy.stats as stats

def mse_with_ci(group, confidence=0.95):
    errors = (group["True CATE"] - group["CATE Estimate"]) ** 2
    mse = errors.mean()
    se = errors.std(ddof=1) / np.sqrt(len(errors))  # Standard error
    z = stats.norm.ppf(1 - (1 - confidence) / 2)  # Z-score for CI
    ci_lower, ci_upper = mse - z * se, mse + z * se
    return pd.Series({"MSE": mse, "SE": se, "CI Lower": ci_lower, "CI Upper": ci_upper})

results = df_all_expanded.groupby(["learning_rate"]).apply(mse_with_ci).reset_index()

results